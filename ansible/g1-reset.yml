- hosts: gluster_nodes
  become: yes
  any_errors_fatal: True

  tasks:

    - name: Build mntpath variable
      set_fact:
        mntpaths: "{{ mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"

    - name: Build arbiter_mntpath variable
      set_fact:
        arbiter_mntpaths: "{{ arbiter_mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/arbiter-{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-arbiter--{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

    - name: Get cache device partition naming convention
      shell: "lsblk -lp | grep part | grep -m 1 {{ item }} | awk '{print $1}'"
      register: result
      with_items: "{{ cache_devices }}"

      #NOTE: Assuming only one cache device; this will break with multiple cache devices
    - name: Isolate any partition prefix characters
      set_fact:
        part_prefix: '{{ result.results.0.stdout.split(cache_devices.0)[1] | default("") | regex_replace("[0-9]+$", "") }}'

    - shell: /bin/bash -c 'echo "Resetting NetworkManager service..." > {{ fifo }}'

    - name: Disable network service
      service: name=network enabled=no
      ignore_errors: true

    - name: Start NetworkManager service
      service: name=NetworkManager state=restarted

    - name: Enable NetworkManager service
      service: name=NetworkManager enabled=yes

    - shell: /bin/bash -c 'echo "Cleaning up any existing Gluster configurations..." > {{ fifo }}'

    - name: Stop glusterd service
      systemd:
        name: glusterd
        state: stopped

    - name: Kill all Gluster processes
      shell: /bin/bash -c 'pkill -9 {{ item }}'
      with_items:
        - glusterfs
        - glusterfsd
      ignore_errors: true

    - name: Clear existing Gluster configuration
      file:
        path: "/var/lib/glusterd/{{ item }}"
        state: absent
      with_items:
        - glusterd.info
        - glustershd
        - bitd
        - glusterfind
        - glustershd
        - nfs
        - peers
        - quotad
        - scrub
        - snaps
        - ss_brick
        - vols
      ignore_errors: true

    - name: Clear any residual Gluster temporary files
      file:
        path: "{{ item }}"
        state: absent
      with_fileglob:
        - "/var/run/gluster/*"
      ignore_errors: true

    - shell: /bin/bash -c 'echo "Cleaning up any existing LVM configurations..." > {{ fifo }}'

    - name: Prime variable in case search is empty
      set_fact:
        our_mounts: []

    - name: Get any of our gluster mounts
      set_fact:
        our_mounts: "{{ our_mounts|default([]) + [item.mount] }}"
      when: "'bricks' in item.mount"
      with_items: "{{ ansible_mounts }}"

    - name: Prime variable in case search is empty
      set_fact:
        our_vgs: []

    - name: Get any of our VGs
      set_fact:
        our_vgs: "{{ our_vgs|default([]) + [item.key] }}"
      when: "'GLVG' in item.key"
      with_dict: "{{ ansible_lvm.vgs }}"

    - name: Get any of our PVs
      shell: "pvs 2>/dev/null | egrep '{{ our_vgs | join('|') }}' | awk '{print $1}'"
      register: pvs_out
      when: our_vgs is defined and (our_vgs|length>0)

    - set_fact:
        our_pvs: "{{ pvs_out.stdout_lines | default([]) }}"

    - name: Unmount gluster bricks
      mount:
        name: "{{ item.0 }}"
        state: "{{ item.1 }}"
      with_nested:
        - "{{ our_mounts }}"
        - ['absent', 'unmounted']

#    - name: Unmount data bricks
#      mount:
#        name: "{{ item.0.path }}"
#        state: "{{ item.1 }}"
#      with_nested:
#        - "{{ mntpaths }}"
#        - ['absent', 'unmounted']
#
#    - name: Unmount arbiter bricks
#      mount:
#        name: "{{ item.0.path }}"
#        state: "{{ item.1 }}"
#      with_nested:
#        - "{{ arbiter_mntpaths }}"
#        - ['absent', 'unmounted']
#      when: arbiter == True

      #- name: Deactivate VGs
      #shell: /bin/bash -c 'vgchange -a n --activationmode partial {{ item.vg }}'
      #with_items: "{{ backend_configuration }}"
      #register: result
      #failed_when: "result.rc != 0 and 'not found' not in result.stderr"

    # we have to try to delete all LVM volumes in reverse order in which they potentially were created before since a simple vgremove --force is not enough since the lvg module is not robust enough - some existing thinpools or VGs still survive this
#    - name: Delete data logical volumes
#      lvol:
#        vg: "{{ item.vg }}"
#        lv: "{{ item.thinLV }}"
#        state: absent
#        force: yes
#      with_items: "{{ backend_configuration }}"
#      ignore_errors: true
#
#    - name: Delete arbiter logical volumes
#      lvol:
#        vg: "{{ item.vg }}"
#        lv: "arbiter-{{ item.thinLV }}"
#        force: yes
#      with_items: "{{ backend_configuration }}"
#      when: arbiter == True
#      ignore_errors: true
#
#    - name: wait for udev to settle
#      shell: udevadm settle --timeout=60
#      changed_when: false
#
#    - name: Clear cache volume
#      lvol:
#        vg: "FAST{{ item.vg }}"
#        lv: "cpool{{ item.id|int + 1 }}"
#        state: "absent"
#        force: yes
#      with_items:
#        - "{{ backend_configuration }}"
#      ignore_errors: true
#
#    - name: wait for udev to settle
#      shell: udevadm settle --timeout=60
#      changed_when: false
#
#    - name: Clear cache volume group
#      lvg:
#        vg: "FAST{{ item.vg }}"
#        state: absent
#        force: yes
#      with_items: "{{ backend_configuration }}"
#      ignore_errors: true
#
#    - name: wait for udev to settle
#      shell: udevadm settle --timeout=60
#      changed_when: false
#
#    - name: Remove data thin pools
#      lvol:
#        vg: "{{ item.vg }}"
#        lv: "{{ item.tp }}"
#        force: yes
#        state: absent
#      with_items: "{{ backend_configuration }}"
#      ignore_errors: true
#
#    - name: wait for udev to settle
#      shell: udevadm settle --timeout=60
#      changed_when: false
#
#    - name: Clear data volume groups
#      lvg:
#        vg: "{{ item.vg }}"
#        state: absent
#        force: yes
#      with_items: "{{ backend_configuration }}"
#      ignore_errors: true

    - name: Clear our volume groups
      lvg:
        vg: "{{ item }}"
        state: absent
        force: yes
      with_items: "{{ our_vgs }}"
      ignore_errors: true

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - shell: /bin/bash -c 'echo "Cleaning up any conflicting device mapper entries..." > {{ fifo }}'

    - name: Gather device mapper entries for data devices
      shell: lsblk {{ item.device }} -lno NAME | grep -v {{ item.device.split('/')[-1] }}
      register: lsblk_data
      with_items: "{{ backend_configuration }}"
      ignore_errors: true

    - set_fact:
        dm_data: "{{ lsblk_data['results'][0]['stdout_lines'] | default([]) }}"

    - name: Gather device mapper entries for cache devices
      shell: lsblk {{ item }} -lno NAME | grep -v {{ item.split('/')[-1] }}
      register: lsblk_cache
      with_items: "{{ cache_devices }}"
      ignore_errors: true

    - set_fact:
        dm_cache: "{{ lsblk_cache['results'][0]['stdout_lines'] | default([]) }}"

    - set_fact:
        dm_devices: "{{ dm_data }} + {{ dm_cache }}"

    - name: Remove any residual device mapper entries
      shell: dmsetup remove {{ item }}
      when: item != ''
      with_items: "{{ dm_devices }}"
      ignore_errors: true
      register: result
      retries: 5
      delay: 2

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - shell: /bin/bash -c 'echo "Wiping cache devices and backend disks..." > {{ fifo }}'

#    - name: Check for actual PVs on the backend disks
#      shell: /bin/bash -c "pvs | grep {{ item.device }} | awk '{print $1}'"
#      with_items: "{{ backend_configuration }}"
#      register: data_pvs
#
#    - name: Remove PV
#      shell: /bin/bash -c 'pvremove --force {{ item }}'
#      register: result
#      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
#      with_items: "{{ data_pvs }}"
#
#    - name: Remove cache PV
#      shell: /bin/bash -c 'pvremove --force {{ item.0 }}{{ part_prefix }}{{ item.1["id"]|int + 1 }}'
#      register: result
#      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
#      with_nested:
#        - "{{ cache_devices }}"
#        - "{{ backend_configuration }}"

    - name: Remove our physical volumes
      shell: 'pvremove --force {{ item }}'
      register: result
      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
      with_items: "{{ our_pvs }}"

    - name: Get cache device information
      shell: "lsblk -lp | grep part | egrep '{{ cache_devices|join('|') }}' | awk '{print $1}'"
      register: lsblk_out

    - name: Cleanup fast device partitions
      shell: "wipefs -af {{ item }}"
      with_items: "{{ lsblk_out.stdout_lines }}"
      register: result
      failed_when: "result.rc != 0 and 'No such file or directory' not in result.stderr"

#    - name: Cleanup cache devices
#      shell: /bin/bash -c '/sbin/wipefs -af {{ item }}'
#      with_items:
#        - "{{ cache_devices }}"
#      register: result
#      failed_when: "result.rc != 0 and 'No such file or directory' not in result.stderr"

    - name: Cleanup data devices
      shell: /bin/bash -c '/sbin/wipefs -af {{ item["device"] }}'
      with_items: "{{ backend_configuration }}"
      register: result
      failed_when: "result.rc != 0 and 'No such file or directory' not in result.stderr"

    - name: Cleanup remaining devices
      shell: /bin/bash -c '/sbin/wipefs -af {{ item }}'
      with_items:
        - "{{ our_pvs }}"
        - "{{ cache_devices }}"
      register: result
      failed_when: "result.rc != 0 and 'No such file or directory' not in result.stderr"
